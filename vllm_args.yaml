# заменить по желанию на:
model: "Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int8"
# model: "Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8"
# model: "Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8"

# заменить по желанию на:
# served_model_name: "Qwen/Qwen2.5-14B-Instruct-1M"

gpu_memory_utilization: 0.7 # каждый гпу будет загружен на 70%. Больше - лучше! Допустимый предел 0 - 1 Подбирать пока не заработает
tensor_parallel_size: 1 # количество гпу, на которых будет крутиться модель параллельно
max_model_len: 9000 # для "Qwen/Qwen2.5-14B-Instruct-1M" установить на 1000000 (или меньше, если не заработает)
quantization: "gptq" # закомментировать для "Qwen/Qwen2.5-14B-Instruct-1M"
seed: 42 # дальше не трогать!
use_v2_block_manager: true
api_key: "NYXV2sS3PLDYLbC"
enable-chunked-prefill: true
max_num_batched_tokens: 2048
# hf_overrides: '{"local_files_only": true}'
swap_space: 2 # int: размер подкачки в ram (tensor_parallel_size * swap_space) = общий объём подкачки (можно уменьшить)
preemption_mode: swap
cpu_offload_gb: 12
